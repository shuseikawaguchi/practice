"""
Main AI Assistant - Integration of all components
"""
import logging
import sys
import json
import re
from pathlib import Path

from config import Config
from llm_manager import LLMManager
from skill_manager import SkillManager
from code_generator import CodeGenerator
from ui_generator import UIGenerator
from model_3d_generator import Model3DGenerator
from idle_mode import IdleMode
from vector_store import VectorStore
from patch_validator import PatchValidator

# Configure logging
_root_logger = logging.getLogger()
_root_logger.setLevel(logging.INFO)
for _h in list(_root_logger.handlers):
    _root_logger.removeHandler(_h)

_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
_file_handler = logging.FileHandler(Config.LOGS_DIR / 'ai_assistant.log')
_file_handler.setLevel(logging.INFO)
_file_handler.setFormatter(_formatter)

_console_handler = logging.StreamHandler()
_console_handler.setLevel(logging.WARNING)
_console_handler.setFormatter(_formatter)

_root_logger.addHandler(_file_handler)
_root_logger.addHandler(_console_handler)

logger = logging.getLogger(__name__)

class AIAssistant:
    def __init__(self):
        logger.info("Initializing AI Assistant...")
        
        # Initialize LLM Manager
        self.llm = LLMManager(model_name=Config.TEACHER_MODEL)
        
        # Check if Ollama is available
        if not self.llm.is_available():
            logger.warning("Warning: Ollama is not available. Please make sure Ollama is running.")
            print("âš ï¸  OllamaãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚OllamaãŒèµ·å‹•ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            print("ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«: https://ollama.ai")
            print("èµ·å‹•: ollama serve")
            print()
        
        # Initialize Skill Manager
        self.skills = SkillManager()
        
        # Initialize Generators
        self.code_gen = CodeGenerator(self.llm)
        self.ui_gen = UIGenerator(self.llm)
        self.model_3d_gen = Model3DGenerator(self.llm)
        self.idle = IdleMode()
        self.vs = VectorStore()
        self._rag_cache = {}
        self._rag_cache_order = []
        
        logger.info("AI Assistant initialized successfully")
    
    def chat(self, user_input: str) -> str:
        """Chat with the AI"""
        logger.info(f"User input: {user_input}")
        response = self.llm.chat(user_input)
        logger.info(f"AI response: {response}")
        return response
    
    def generate_code(self, requirement: str, language: str = "python") -> str:
        """Generate code"""
        logger.info(f"Generating {language} code for: {requirement}")
        if language.lower() == "python":
            code = self.code_gen.generate_python(requirement)
        elif language.lower() == "javascript":
            code = self.code_gen.generate_javascript(requirement)
        elif language.lower() == "html":
            code = self.code_gen.generate_html(requirement)
        else:
            code = self.code_gen.generate_from_description(requirement, language)
        
        # Mark skill as used
        self.skills.improve_skill("code_generation", 0.05)
        return code
    
    def generate_ui(self, description: str) -> str:
        """Generate UI"""
        logger.info(f"Generating UI: {description}")
        html = self.ui_gen.generate_html_ui(description)
        self.skills.improve_skill("ui_generation", 0.05)
        return html
    
    def generate_3d_model(self, description: str) -> str:
        """Generate 3D model"""
        logger.info(f"Generating 3D model: {description}")
        model = self.model_3d_gen.generate_threejs_scene(description)
        self.skills.improve_skill("model_3d_generation", 0.05)
        return model

    def summarize_text(self, text: str) -> str:
        """Summarize a given text"""
        logger.info("Summarizing text")
        prompt = (
            "ä»¥ä¸‹ã®æ–‡ç« ã‚’æ—¥æœ¬èªã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚é‡è¦ãªè¦ç‚¹ã‚’3ã€œ5å€‹ã®ç®‡æ¡æ›¸ãã§ç¤ºã—ã€"
            "æœ€å¾Œã«1è¡Œã®çµè«–ã‚’ä»˜ã‘ã¦ãã ã•ã„ã€‚\n\n"
            f"æ–‡ç« :\n{text}\n\nè¦ç´„ï¼š"
        )
        return self.llm.generate(prompt, temperature=0.2, max_tokens=400)

    def generate_plan(self, requirement: str) -> str:
        """Generate a concise implementation plan"""
        logger.info("Generating plan")
        prompt = (
            "ä»¥ä¸‹ã®è¦ä»¶ã«å¯¾ã™ã‚‹å®Ÿè£…è¨ˆç”»ã‚’æ—¥æœ¬èªã§ä½œæˆã—ã¦ãã ã•ã„ã€‚"
            "ã‚¹ãƒ†ãƒƒãƒ—ç•ªå·ä»˜ãã§ã€æœ€å°3ã‚¹ãƒ†ãƒƒãƒ—ã€œæœ€å¤§8ã‚¹ãƒ†ãƒƒãƒ—ã«ã—ã¦ãã ã•ã„ã€‚\n\n"
            f"è¦ä»¶:\n{requirement}\n\nè¨ˆç”»ï¼š"
        )
        return self.llm.generate(prompt, temperature=0.2, max_tokens=400)

    def rag_query(self, query: str, k: int = 4) -> str:
        """Retrieve relevant documents and ask LLM to answer using context (RAG)."""
        cache_key = f"{k}|{query}"
        cached = self._rag_cache_get(cache_key)
        if cached is not None:
            return cached
        try:
            # ensure index loaded
            self.vs.load()
        except Exception:
            pass

        try:
            results = self.vs.query(query, k=k)
        except Exception:
            results = []

        context = ''
        for r in results:
            context += f"[Source: {r['meta'].get('source','')}]\n{r['text']}\n\n"

        prompt = f"ä»¥ä¸‹ã®å‚è€ƒè³‡æ–™ã‚’å‚ç…§ã—ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•ã„ã€‚\n\nå‚è€ƒè³‡æ–™:\n{context}\nè³ªå•:\n{query}\n\nå›ç­”ï¼š"
        answer = self.llm.generate(prompt, temperature=0.2, max_tokens=800)
        self._rag_cache_set(cache_key, answer)
        return answer

    def _rag_cache_get(self, key: str):
        if key in self._rag_cache:
            try:
                self._rag_cache_order.remove(key)
            except ValueError:
                pass
            self._rag_cache_order.append(key)
            return self._rag_cache[key]
        return None

    def _rag_cache_set(self, key: str, value: str):
        if not value:
            return
        if key in self._rag_cache:
            self._rag_cache[key] = value
            try:
                self._rag_cache_order.remove(key)
            except ValueError:
                pass
            self._rag_cache_order.append(key)
            return
        self._rag_cache[key] = value
        self._rag_cache_order.append(key)
        limit = Config.RAG_CACHE_SIZE
        if limit and len(self._rag_cache_order) > limit:
            old = self._rag_cache_order.pop(0)
            self._rag_cache.pop(old, None)
    
    def get_status(self) -> dict:
        """Get AI status and skill information"""
        return {
            "llm_available": self.llm.is_available(),
            "skills": self.skills.get_status(),
            "memory_file": str(Config.MEMORY_FILE),
            "log_file": str(Config.LOGS_DIR / 'ai_assistant.log')
        }
    
    def interactive_chat(self):
        """Interactive chat mode"""
        print("\nğŸ¤– AI Assistant - Interactive Mode")
        print("=" * 50)
        print("ã‚³ãƒãƒ³ãƒ‰:")
        print("  /help        - ãƒ˜ãƒ«ãƒ—è¡¨ç¤º")
        print("  /code        - ã‚³ãƒ¼ãƒ‰ç”Ÿæˆãƒ¢ãƒ¼ãƒ‰")
        print("  /ui          - UIç”Ÿæˆãƒ¢ãƒ¼ãƒ‰")
        print("  /3d          - 3Dãƒ¢ãƒ‡ãƒ«ç”Ÿæˆãƒ¢ãƒ¼ãƒ‰")
        print("  /status      - ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹è¡¨ç¤º")
        print("  /patch-list  - ææ¡ˆãƒ‘ãƒƒãƒä¸€è¦§")
        print("  /patch-approve <id> - ãƒ‘ãƒƒãƒã‚’æ‰¿èª")
        print("  /clear       - ä¼šè©±å±¥æ­´ã‚¯ãƒªã‚¢")
        print("  /exit        - çµ‚äº†")
        print("=" * 50)
        
        while True:
            try:
                user_input = input("\nã‚ãªãŸ: ").strip()
                
                if not user_input:
                    continue
                
                if user_input == "/exit":
                    print("ã•ã‚ˆã†ãªã‚‰ï¼")
                    break
                elif user_input in ("æ”¾ç½®ãƒ¢ãƒ¼ãƒ‰ã‚’ã‚ªãƒ³ã«ã—ã¦ãã ã•ã„", "æ”¾ç½®ãƒ¢ãƒ¼ãƒ‰ã‚’é–‹å§‹ã—ã¦", "/idle-on"):
                    res = self.idle.start_idle()
                    print(f"æ”¾ç½®ãƒ¢ãƒ¼ãƒ‰é–‹å§‹: {res}")
                elif user_input in ("æ”¾ç½®ãƒ¢ãƒ¼ãƒ‰ã‚’åœæ­¢ã—ã¦ãã ã•ã„", "æ”¾ç½®ãƒ¢ãƒ¼ãƒ‰ã‚’ã‚ªãƒ•ã«ã—ã¦ãã ã•ã„", "/idle-off"):
                    res = self.idle.stop_idle()
                    print(f"æ”¾ç½®ãƒ¢ãƒ¼ãƒ‰åœæ­¢: {res}")
                elif user_input in ("æ”¾ç½®ãƒ¢ãƒ¼ãƒ‰ã®çŠ¶æ…‹ã‚’æ•™ãˆã¦", "/idle-status"):
                    st = self.idle.status()
                    print(f"æ”¾ç½®ãƒ¢ãƒ¼ãƒ‰çŠ¶æ…‹: {st}")
                elif user_input == "/help":
                    self._show_help()
                elif user_input == "/status":
                    self._show_status()
                elif user_input.startswith("/patch-list"):
                    self._show_patch_list()
                elif user_input.startswith("/patch-approve"):
                    parts = user_input.split()
                    if len(parts) > 1:
                        patch_id = parts[1]
                        self._approve_patch(patch_id)
                    else:
                        print("ç”¨æ³•: /patch-approve <patch_id>")
                elif user_input == "/clear":
                    self.llm.clear_history()
                    print("âœ… ä¼šè©±å±¥æ­´ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ")
                elif user_input == "/code":
                    self._code_generation_mode()
                elif user_input == "/ui":
                    self._ui_generation_mode()
                elif user_input == "/3d":
                    self._3d_generation_mode()
                else:
                    self._execute_and_apply(user_input)
                    
            except KeyboardInterrupt:
                print("\n\nã•ã‚ˆã†ãªã‚‰ï¼")
                break
            except Exception as e:
                logger.error(f"Error: {e}")
                print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    
    def _code_generation_mode(self):
        """Code generation mode"""
        print("\nğŸ’» Code Generation Mode")
        print("è¨€èª: python, javascript, html")
        language = input("è¨€èªã‚’é¸æŠã—ã¦ãã ã•ã„ (python): ").strip() or "python"
        requirement = input("ç”Ÿæˆã—ã¦ã»ã—ã„ã‚³ãƒ¼ãƒ‰ã®èª¬æ˜: ").strip()
        
        if requirement:
            print("\nâ³ ã‚³ãƒ¼ãƒ‰ã‚’ç”Ÿæˆä¸­...")
            code = self.generate_code(requirement, language)
            print(f"\nç”Ÿæˆã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰:\n{code}")
    
    def _ui_generation_mode(self):
        """UI generation mode"""
        print("\nğŸ¨ UI Generation Mode")
        description = input("UIã®èª¬æ˜: ").strip()
        
        if description:
            print("\nâ³ UIã‚’ç”Ÿæˆä¸­...")
            html = self.generate_ui(description)
            print(f"\nç”Ÿæˆã•ã‚ŒãŸUI:\n{html}")

    def _ui_apply_mode(self, instruction: str):
        """Apply UI improvements directly to api.py"""
        target = Config.PROJECT_ROOT / "api.py"
        if not target.exists():
            print("âŒ api.py ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return
        original = target.read_text(encoding="utf-8", errors="ignore")
        prompt = (
            "æ¬¡ã®UIãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŒ‡å®šã®æŒ‡ç¤ºã§æ”¹è‰¯ã—ã¦ãã ã•ã„ã€‚å‡ºåŠ›ã¯JSONã®ã¿ã€‚\n"
            "åˆ¶ç´„: æ—¢å­˜æ©Ÿèƒ½ã‚’å£Šã•ãªã„/å¤‰æ›´ã¯UIé ˜åŸŸã®ã¿/ã§ãã‚‹ã ã‘å°ã•ãã€‚\n"
            "å½¢å¼: {\"title\":...,\"description\":...,\"files\":{\"api.py\":\"content\"}}\n"
            f"æŒ‡ç¤º: {instruction}\n\n"
            "--- ç¾åœ¨ã®å†…å®¹ ---\n"
            f"{original}\n"
        )
        print("\nâ³ UIæ”¹è‰¯ã‚’é©ç”¨ä¸­...")
        response = self.llm.generate(prompt, temperature=0.2, max_tokens=900)
        data = self._extract_json(response)
        if not data:
            print("âŒ JSONè§£æã«å¤±æ•—ã—ã¾ã—ãŸ")
            return
        files = data.get("files") or {}
        if "api.py" not in files:
            print("âŒ api.py ã®å¤‰æ›´ãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
            return
        title = data.get("title") or "UI auto-improve"
        description = data.get("description") or instruction
        try:
            proposal = PatchValidator.create_and_validate(title, description, files, auto_propose=True)
            print(f"âœ… é©ç”¨å®Œäº†: {proposal.id} ({proposal.status})")
        except Exception as e:
            print(f"âŒ é©ç”¨å¤±æ•—: {e}")

    def _execute_and_apply(self, instruction: str):
        """Execute instruction by generating and applying a patch."""
        print("\nâ³ æŒ‡ç¤ºã‚’å®Ÿè¡Œä¸­...")
        candidates = self._find_relevant_files(instruction, max_files=4)
        if not candidates:
            print("âŒ é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return

        file_blocks = []
        for path in candidates:
            try:
                content = (Config.PROJECT_ROOT / path).read_text(encoding="utf-8", errors="ignore")
            except Exception:
                continue
            if len(content) > 20000:
                content = content[:20000]
            file_blocks.append(f"--- {path} ---\n{content}\n")

        prompt = (
            "æ¬¡ã®æŒ‡ç¤ºã‚’å®Ÿè¡Œã™ã‚‹ãŸã‚ã€å¿…è¦æœ€å°é™ã®å¤‰æ›´ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚å‡ºåŠ›ã¯JSONã®ã¿ã€‚\n"
            "åˆ¶ç´„: æ—¢å­˜æ©Ÿèƒ½ã‚’å£Šã•ãªã„/å¤‰æ›´ã¯æœ€å°é™/å¯¾è±¡ã¯æç¤ºã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ã€‚\n"
            "å½¢å¼: {\"title\":...,\"description\":...,\"files\":{\"path\":\"content\"}}\n"
            f"æŒ‡ç¤º: {instruction}\n\n"
            + "\n".join(file_blocks)
        )

        response = self.llm.generate(prompt, temperature=0.2, max_tokens=1200)
        data = self._extract_json(response)
        if not data:
            print("âŒ JSONè§£æã«å¤±æ•—ã—ã¾ã—ãŸ")
            return
        files = data.get("files") or {}
        if not isinstance(files, dict) or not files:
            print("âŒ å¤‰æ›´ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")
            return

        title = data.get("title") or "Auto execute"
        description = data.get("description") or instruction
        try:
            proposal = PatchValidator.create_and_validate(title, description, files, auto_propose=True)
            print(f"âœ… é©ç”¨å®Œäº†: {proposal.id} ({proposal.status})")
        except Exception as e:
            print(f"âŒ é©ç”¨å¤±æ•—: {e}")
    
    def _3d_generation_mode(self):
        """3D model generation mode"""
        print("\nğŸª 3D Model Generation Mode")
        description = input("3Dãƒ¢ãƒ‡ãƒ«ã®èª¬æ˜: ").strip()
        
        if description:
            print("\nâ³ 3Dãƒ¢ãƒ‡ãƒ«ã‚’ç”Ÿæˆä¸­...")
            model = self.generate_3d_model(description)
            print(f"\nç”Ÿæˆã•ã‚ŒãŸ3Dãƒ¢ãƒ‡ãƒ«:\n{model}")
    
    def _show_help(self):
        """Show help"""
        help_text = """
ğŸ¤– AI Assistant ãƒ˜ãƒ«ãƒ—

é€šå¸¸ã®ãƒãƒ£ãƒƒãƒˆ:
  AIã«è³ªå•ã‚„æŒ‡ç¤ºã‚’å…¥åŠ›ã—ã¦ãã ã•ã„

ã‚³ãƒãƒ³ãƒ‰:
  /code   - Python/JavaScript/HTMLã‚³ãƒ¼ãƒ‰ç”Ÿæˆãƒ¢ãƒ¼ãƒ‰
    /ui     - HTMLã¨CSSã®UIç”Ÿæˆãƒ¢ãƒ¼ãƒ‰
  /3d     - Three.jsã®3Dãƒ¢ãƒ‡ãƒ«ç”Ÿæˆãƒ¢ãƒ¼ãƒ‰
  /status - AIã®èƒ½åŠ›ã¨ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’è¡¨ç¤º
  /clear  - ä¼šè©±å±¥æ­´ã‚’ã‚¯ãƒªã‚¢
  /help   - ã“ã®ãƒ˜ãƒ«ãƒ—ã‚’è¡¨ç¤º
  /exit   - çµ‚äº†

æ©Ÿèƒ½:
  â€¢ æ—¥æœ¬èªã§ã®è‡ªç„¶ãªä¼šè©±
  â€¢ Pythonã‚³ãƒ¼ãƒ‰ç”Ÿæˆ
  â€¢ JavaScriptã‚³ãƒ¼ãƒ‰ç”Ÿæˆ
  â€¢ HTMLã¨CSSç”Ÿæˆ
  â€¢ Three.jsã§ã®3Dãƒ¢ãƒ‡ãƒ«ä½œæˆ
  â€¢ æ®µéšçš„ãªå­¦ç¿’ã¨æ”¹å–„
"""
        print(help_text)

    def _extract_json(self, text: str) -> dict | None:
        if not text:
            return None
        cleaned = text.strip()
        if cleaned.startswith("```"):
            parts = cleaned.split("```")
            if len(parts) >= 2:
                cleaned = parts[1].strip()
                if cleaned.startswith("json"):
                    cleaned = cleaned[4:].strip()
        try:
            obj = json.loads(cleaned)
            return obj if isinstance(obj, dict) else None
        except Exception:
            pass
        start = cleaned.find("{")
        while start != -1:
            end = cleaned.find("}", start)
            while end != -1:
                candidate = cleaned[start:end + 1]
                try:
                    obj = json.loads(candidate)
                    if isinstance(obj, dict):
                        return obj
                except Exception:
                    pass
                end = cleaned.find("}", end + 1)
            start = cleaned.find("{", start + 1)
        return None

    def _find_relevant_files(self, instruction: str, max_files: int = 4) -> list[str]:
        tokens = [t for t in re.split(r"\s+", instruction) if t]
        keywords = [t for t in tokens if len(t) >= 2][:8]
        exts = set(Config.AUTO_PATCH_CANDIDATE_EXTS or [".py"])
        exclude_dirs = set(Config.AUTO_PATCH_EXCLUDE_DIRS or [])
        hits = []

        for p in Config.PROJECT_ROOT.rglob("*"):
            if p.is_dir():
                if p.name in exclude_dirs:
                    continue
                continue
            if p.suffix not in exts:
                continue
            parts = set(p.parts)
            if parts.intersection(exclude_dirs):
                continue
            rel = p.relative_to(Config.PROJECT_ROOT).as_posix()
            try:
                content = p.read_text(encoding="utf-8", errors="ignore")
            except Exception:
                continue
            score = 0
            for kw in keywords:
                if kw in content or kw in rel:
                    score += 1
            if score > 0:
                hits.append((score, rel))
            if len(hits) > 200:
                break

        hits.sort(key=lambda x: x[0], reverse=True)
        return [h[1] for h in hits[:max_files]]
    
    def _show_status(self):
        """Show AI status"""
        status = self.get_status()
        print("\nğŸ“Š AI Assistant Status")
        print("=" * 50)
        print(f"LLMåˆ©ç”¨å¯èƒ½: {'âœ… ã¯ã„' if status['llm_available'] else 'âŒ ã„ã„ãˆ'}")
        
        skills = status['skills']
        print(f"\nç¿’å¾—ã‚¹ã‚­ãƒ«æ•°: {skills['learned_skills']}/{skills['total_skills']}")
        print(f"å¹³å‡ç²¾åº¦: {skills['average_accuracy']:.1%}")
        
        print("\nã‚¹ã‚­ãƒ«è©³ç´°:")
        for skill_name, skill_info in skills['skills'].items():
            status_icon = "âœ…" if skill_info['is_learned'] else "âŒ"
            print(f"  {status_icon} {skill_name}: {skill_info['accuracy']:.1%} (ä½¿ç”¨å›æ•°: {skill_info['usage_count']})")
        
        print("=" * 50)

    def _show_patch_list(self):
        """Show list of patches"""
        proposals = PatchValidator.list_proposals()
        print("\nğŸ“‹ ãƒ‘ãƒƒãƒææ¡ˆä¸€è¦§")
        print("=" * 70)
        if not proposals:
            print("ãƒ‘ãƒƒãƒãªã—")
        else:
            for p in proposals:
                status_icon = "ğŸ†—" if p['status'] == 'APPROVED' else "â³" if p['status'] == 'PROPOSED' else "âŒ"
                print(f"{status_icon} {p['id']}")
                print(f"   ã‚¿ã‚¤ãƒˆãƒ«: {p['title']}")
                print(f"   ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {p['status']}")
                print(f"   ãƒ•ã‚¡ã‚¤ãƒ«: {', '.join(p['files'])}")
                print()
        print("=" * 70)

    def _approve_patch(self, patch_id: str):
        """Approve a patch"""
        if PatchValidator.approve_proposal(patch_id):
            print(f"âœ… ãƒ‘ãƒƒãƒ {patch_id} ã‚’æ‰¿èªã—ã¾ã—ãŸ")
        else:
            print(f"âŒ ãƒ‘ãƒƒãƒ {patch_id} ã‚’æ‰¿èªã§ãã¾ã›ã‚“ã§ã—ãŸ")

def main():
    """Main entry point"""
    assistant = AIAssistant()
    assistant.interactive_chat()

if __name__ == "__main__":
    main()
